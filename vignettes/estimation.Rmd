---
title: "Model estimation"
author: David Selby
date: August 2022
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here we implement the AOP(1) model of Mo Li and QiQi Lu in the paper https://doi.org/10.1002/env.2752, published in _Environmetrics_ in 2022.
The paper proposes a latent Gaussian process model for ordinal data.
And several CUSUM statistics for detecting changepoints.

We minimize
\[Q(\boldsymbol\theta) = \sum_{t=1}^n \sum_{k=1}^K (Y_{t,k} - \mathbb{E}[Y_{t,k}])^2,\]
where
\[\mathbb{E}[Y_{tk}] = \Phi(c_k - \mu_t) - \Phi(c_{k-1} - \mu_t).\]

The latent variable $Z_t$ is assumed to have unit variance so that the sum of squares objective function $Q$ does not depend on the autocorrelation parameter $\rho$.

We use Newton's method with a tolerance of $10^{-5}$.
This is implemented in the `nlm()` function in R.
Starting values for $\alpha_1, A, D, \Delta$ are set to 0.
Starting values for $\alpha_0$ and thresholds $c_2,\dots,c_{K-1}$ are $\alpha_0 = -\Phi^{-1}(p_1)$ and $c_k=-\Phi^{-1}\left(\sum_{j=1}^k p_j\right) + \alpha_0, where $p_j$ is the sample proportion of times that $Y_t$ lies in category $j$.

Recall the model definition
\[
\mu_t = \alpha_0 + \alpha_1 \frac{t}{n} + s_t + \Delta I_{(t \geq \tau)},
\]
where
\[s_t = A \cos\left(\frac{2\pi}{T} t\right) + D \sin \left(\frac{2\pi}{T}t\right)\]
and we have parameter vector $\boldsymbol\theta = (c_2, \dots, c_{K-1}, \alpha_0, \alpha_1, A, D, \Delta)$.

## Simulation

```{r}
local({
  n <- 10000
  #  K <- 3
  L <- 3
  alpha0 <- 0.4307
  alpha1 <- 0.1
  A <- 0.3
  D <- -0.5
  rho <- 0.5
  t <- 1:n
  s <- A * cos(2 * pi / L * t) + D * sin(2 * pi / L * t)
  mu <- alpha0 + alpha1 * t / n + s # no changepoints yet
  Z <- rnorm(n, mu, sqrt(1 - rho^2))
  for (i in 2:n)
    Z[i] <- Z[i] + rho * (Z[i - 1] - mu[i - 1])
  Y <- findInterval(Z, c(0, 0.8615)) + 1
  Y
}) -> Y

library(ordinalchange)
seq_estimation(Y, tau = 5000, K = 3, L = 3)
# ignore the last element of theta since we don't have any changepoints
# should be (0.8615, 0.4307, 0.1, 0.3, -0.5, ___)
# and rho should be 0.5
```

```{r}
local({
  n <- 10000
  #  K <- 3
  L <- 3
  alpha0 <- 0.4307
  alpha1 <- 0.1
  A <- 0.3
  D <- -0.5
  rho <- -0.5
  t <- 1:n
  s <- A * cos(2 * pi / L * t) + D * sin(2 * pi / L * t)
  mu <- alpha0 + alpha1 * t / n + s # no changepoints yet
  Z <- rnorm(n, mu, sqrt(1 - rho^2))
  for (i in 2:n)
    Z[i] <- Z[i] + rho * (Z[i - 1] - mu[i - 1])
  Y <- findInterval(Z, c(0, 0.8615)) + 1
  Y
}) -> Y

seq_estimation(Y, tau = 5000, K = 3, L = 3)
# as above but with rho = -0.5
```

## CUSUM test statistics

Currently the test statistic $M_X$ is implemented. Others may be added in the future.

```{r}
changepoint_detection(Y, L = 3, t = seq(2, length(Y) - 1, by = 1000),
                      verbose = TRUE)
```

```{r}
local({
  n <- 1000
  # K <- 3
  L <- 3
  alpha0 <- 0.4307
  alpha1 <- 0.1
  A <- 0.3
  D <- -0.5
  rho <- -0.5
  Delta <- -0.7
  tau <- 300
  t <- 1:n
  s <- A * cos(2 * pi / L * t) + D * sin(2 * pi / L * t)
  mu <- alpha0 + alpha1 * t / n + s +
      Delta * (t >= tau) # changepoint
  Z <- rnorm(n, mu, sqrt(1 - rho^2))
  for (i in 2:n)
    Z[i] <- Z[i] + rho * (Z[i - 1] - mu[i - 1])
  Y <- findInterval(Z, c(0, 0.8615)) + 1
  Y
}) -> Y

changepoint_detection(Y, L = 3, t = c(2, 50, seq(100, length(Y), by = 100)),
                      verbose = TRUE)
```

## Equivalent model in stan

Basing this on https://mc-stan.org/docs/stan-users-guide/change-point.html

But it is very inefficient at the moment so it might be better either to use Julia (for Gibbs sampling) or just not do this at all, treating changepoints as a 'bad idea'.

```{r eval = T}
model_code <- "data {
  int<lower=1> T; // Length of time series
  int<lower=1> K; // Number of response categories
  int<lower=1, upper=K> Y[T]; // Observations
  int<lower=1> Period; // Known period of seasonal component
}

transformed data {
  real log_unif = -log(T);
}

parameters {
  real A;
  real D;
  real alpha0;
  real alpha1;
  real Delta; // level shift at changepoint
  ordered[K - 2] threshold;
  real<lower=-1,upper=1> rho;
}

transformed parameters {
  vector[K-1] cutpoints = append_row(0, threshold); // for identifiability
  vector[T] lp;
  lp = rep_vector(log_unif, T);
  {
    for (cp in 1:T) {
      vector[T] Z;
      vector[T] mu;
      for (t in 1:T) {
        real s;
        s = A * cos(2 * pi() / Period * t) + D * sin(2 * pi() / Period * t);
        mu[t] = alpha0 + alpha1 * (t * 1.0 / T) + s + (t >= cp ? Delta : 0);
        if (t == 1) {
          Z[t] = mu[t];
        } else {
          Z[t] = mu[t] + rho * (Z[t - 1] - mu[t - 1]);
        }
        lp[cp] = lp[cp] + ordered_probit_lpmf(Y[t] | Z[t], cutpoints);
      }
    }
  }
}

model {
  A ~ normal(0, 3);
  D ~ normal(0, 3);
  alpha0 ~ normal(0, 3);
  alpha1 ~ normal(0, 3);
  Delta ~ normal(0, 3);
  threshold ~ normal(0, 3);
  target += log_sum_exp(lp);
}

//generated quantities {
//  int<lower=1, upper=T> tau;
//  tau = categorical_logit_rng(lp); // Highly inefficient. Just use softmax(lp)??
//}"

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
model <- stan_model(model_code = model_code)
```

```{r eval = T}
result <- optimizing(model, data = list(Y = Y, T = length(Y), K = 3, Period = 3))
lp <- result$theta_tilde[, paste0('lp[', seq_along(Y), ']')]
softmax <- function(x) exp(x) / sum(exp(x))
plot(softmax(lp), xlab = 't', main = 'Stan changepoint algorithm output')
```